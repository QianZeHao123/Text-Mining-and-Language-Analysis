{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\QianZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg  # Import the gutenberg corpus from NLTK\n",
    "import nltk  # Import the NLTK library\n",
    "nltk.download('gutenberg')  # Download the gutenberg corpus\n",
    "gutenberg.fileids()  # List file-ids in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars\tWords\tSents\tFile\n",
      "   7752\t 192427\t 887071\tausten-emma.txt\n",
      "   3747\t  98171\t 466292\tausten-persuasion.txt\n",
      "   4999\t 141576\t 673022\tausten-sense.txt\n",
      "  30103\t1010654\t4332554\tbible-kjv.txt\n",
      "    438\t   8354\t  38153\tblake-poems.txt\n",
      "   2863\t  55563\t 249439\tbryant-stories.txt\n",
      "   1054\t  18963\t  84663\tburgess-busterbrown.txt\n",
      "   1703\t  34110\t 144395\tcarroll-alice.txt\n",
      "   4779\t  96996\t 457450\tchesterton-ball.txt\n",
      "   3806\t  86063\t 406629\tchesterton-brown.txt\n",
      "   3742\t  69213\t 320525\tchesterton-thursday.txt\n",
      "  10230\t 210663\t 935158\tedgeworth-parents.txt\n",
      "  10059\t 260819\t1242990\tmelville-moby_dick.txt\n",
      "   1851\t  96825\t 468220\tmilton-paradise.txt\n",
      "   2163\t  25833\t 112310\tshakespeare-caesar.txt\n",
      "   3106\t  37360\t 162881\tshakespeare-hamlet.txt\n",
      "   1907\t  23140\t 100351\tshakespeare-macbeth.txt\n",
      "   4250\t 154883\t 711215\twhitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Chars\\tWords\\tSents\\tFile\")\n",
    "for fileid in gutenberg.fileids(): # Iterate through files in corpus\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    print(\"%7.0f\\t%7.0f\\t%7.0f\\t%s\" % (num_sents,num_words,num_chars,fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listofprocessedwords: ['the', 'new', 'table', 'is', 'red', 'the', 'blue', 'table', 'is', 'broken']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize  # Importtheword_tokenizefunctionfromNLTK\n",
    "from string import punctuation\n",
    "punctuation_list = list(punctuation)  # Convertstringwithpunctuationmarkstolist\n",
    "text = \"The new table is red. The blue table is broken.\"\n",
    "text_tokens_processed = []\n",
    "text_tokens = word_tokenize(text)  # Tokenisethetextintowords\n",
    "for token in text_tokens:  # Iteratethroughtheavailabletokens\n",
    "    if token not in punctuation_list:  # Omittokensthatarepunctuationmarks\n",
    "        # Addlowercaseversionoftokentolist\n",
    "        text_tokens_processed.append(token.lower())\n",
    "print(\"List of processed words:\", text_tokens_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the', 'new', 'red', 'table', 'is', 'broken', 'blue'}\n",
      "Vocabularysize: 7\n",
      "\n",
      "Vocabulary2: {'the', 'new', 'red', 'table', 'is', 'broken', 'blue'}\n",
      "Vocabulary2size: 7\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()  # Createanemptyset\n",
    "for word in text_tokens_processed:  # Iteratethroughavailablewords\n",
    "    vocabulary.add(word)  # Addwordtoset\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Vocabularysize:\", len(vocabulary))\n",
    "vocabulary2 = set(text_tokens_processed)\n",
    "print(\"\\nVocabulary2:\", vocabulary2)\n",
    "print(\"Vocabulary2size:\", len(vocabulary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7328\tausten-emma.txt\n",
      "  5820\tausten-persuasion.txt\n",
      "  6388\tausten-sense.txt\n",
      " 12755\tbible-kjv.txt\n",
      "  1521\tblake-poems.txt\n",
      "  3925\tbryant-stories.txt\n",
      "  1547\tburgess-busterbrown.txt\n",
      "  2622\tcarroll-alice.txt\n",
      "  8313\tchesterton-ball.txt\n",
      "  7780\tchesterton-brown.txt\n",
      "  6335\tchesterton-thursday.txt\n",
      "  8432\tedgeworth-parents.txt\n",
      " 17215\tmelville-moby_dick.txt\n",
      "  9007\tmilton-paradise.txt\n",
      "  3019\tshakespeare-caesar.txt\n",
      "  4703\tshakespeare-hamlet.txt\n",
      "  3451\tshakespeare-macbeth.txt\n",
      " 12437\twhitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():  # Iterate through documents in corpus\n",
    "    vocabulary_of_document = set()  # Create empty set\n",
    "    for word in gutenberg.words(fileid):  # Iterate through words in document\n",
    "        if word not in punctuation_list:  # Omit tokens that are punctuation marks\n",
    "            vocabulary_of_document.add(word.lower())\n",
    "    print(\"%6.0f\\t%s\" % (len(vocabulary_of_document), fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of Gutenberg corpus: 42314 words\n"
     ]
    }
   ],
   "source": [
    "vocabulary_of_corpus = set() # Create empty set\n",
    "\n",
    "for fileid in gutenberg.fileids(): # Iterate through documents in corpus\n",
    "    for word in gutenberg.words(fileid): # Iterate through words in document\n",
    "        if word not in punctuation_list: # Omit tokens that are punctuation marks\n",
    "            vocabulary_of_corpus.add(word.lower())\n",
    "\n",
    "print(\"Vocabulary of Gutenberg corpus:\",len(vocabulary_of_corpus),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'table' 'blue' 'red' 'broken' 'new' 'the'] \n",
      "\n",
      "[2 5 0 4 1 3 6] \n",
      "\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]] \n",
      "\n",
      "[0. 0. 1. 0. 0. 0. 0.] -> is\n",
      "[0. 0. 0. 0. 0. 1. 0.] -> table\n",
      "[1. 0. 0. 0. 0. 0. 0.] -> blue\n",
      "[0. 0. 0. 0. 1. 0. 0.] -> red\n",
      "[0. 1. 0. 0. 0. 0. 0.] -> broken\n",
      "[0. 0. 0. 1. 0. 0. 0.] -> new\n",
      "[0. 0. 0. 0. 0. 0. 1.] -> the\n"
     ]
    }
   ],
   "source": [
    "from numpy import array  # Import array type from numpy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "vocabulary = ['is', 'table', 'blue', 'red', 'broken', 'new', 'the']\n",
    "# Convert to array because it is required by the LabelEncoder() object\n",
    "data = array(vocabulary)\n",
    "print(data, \"\\n\")\n",
    "# Integer encoding- Assigns a unique index to each unique word\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(data)\n",
    "print(integer_encoded, \"\\n\")\n",
    "# One-Hot encoding- Assigns a One-Hot binary representation to each word\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded, \"\\n\")\n",
    "for i in range(len(data)):\n",
    "    print(onehot_encoded[i], \"->\", data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': array([0., 0., 1., 0., 0., 0., 0.]), 'table': array([0., 0., 0., 0., 0., 1., 0.]), 'blue': array([1., 0., 0., 0., 0., 0., 0.]), 'red': array([0., 0., 0., 0., 1., 0., 0.]), 'broken': array([0., 1., 0., 0., 0., 0., 0.]), 'new': array([0., 0., 0., 1., 0., 0., 0.]), 'the': array([0., 0., 0., 0., 0., 0., 1.])}\n",
      "\n",
      "red = [0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "for i in range(len(data)):\n",
    "    dictionary[data[i]] = onehot_encoded[i]\n",
    "\n",
    "print(dictionary)\n",
    "print(\"\\nred =\",dictionary['red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "def get_label_from_dictionary(dictionary,value):\n",
    "    for word, one_hot in dictionary.items(): # Iterate all pairs (word, one-hot representation) in the dictionary\n",
    "        if (one_hot == value).all(): # Compare equality between numpy arrays\n",
    "            return word\n",
    "print(get_label_from_dictionary(dictionary,[0,0,0,0,1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of processed words: ['the', 'new', 'table', 'is', 'red', 'the', 'blue', 'table', 'is', 'broken']\n",
      "[0 0 0 0 0 0 0] OR [0. 0. 0. 0. 0. 0. 1.] = [0 0 0 0 0 0 1]\n",
      "[0 0 0 0 0 0 1] OR [0. 0. 0. 1. 0. 0. 0.] = [0 0 0 1 0 0 1]\n",
      "[0 0 0 1 0 0 1] OR [0. 0. 0. 0. 0. 1. 0.] = [0 0 0 1 0 1 1]\n",
      "[0 0 0 1 0 1 1] OR [0. 0. 1. 0. 0. 0. 0.] = [0 0 1 1 0 1 1]\n",
      "[0 0 1 1 0 1 1] OR [0. 0. 0. 0. 1. 0. 0.] = [0 0 1 1 1 1 1]\n",
      "[0 0 1 1 1 1 1] OR [0. 0. 0. 0. 0. 0. 1.] = [0 0 1 1 1 1 1]\n",
      "[0 0 1 1 1 1 1] OR [1. 0. 0. 0. 0. 0. 0.] = [1 0 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1] OR [0. 0. 0. 0. 0. 1. 0.] = [1 0 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1] OR [0. 0. 1. 0. 0. 0. 0.] = [1 0 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1] OR [0. 1. 0. 0. 0. 0. 0.] = [1 1 1 1 1 1 1]\n",
      "\n",
      "One-Hot encoded text: [1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from numpy import logical_or # Import the element-wise logical OR function from numpy\n",
    "from numpy import zeros # Import the zeros function from numpy\n",
    "text = \"The new table is red. The blue table is broken.\"\n",
    "print(\"List of processed words:\",text_tokens_processed)\n",
    "result = zeros(len(dictionary)) # Start from zero-valued vector- Convert to numpy array\n",
    "for word in text_tokens_processed: # Iterate words in text\n",
    "    print(result.astype(int), \"OR\", dictionary[word],\"= \",end='')\n",
    "    result = logical_or(result,dictionary[word]) # Compute the element-wise logical or between the partial result and the one-hot representation of the word\n",
    "    print(result.astype(int))\n",
    "print(\"\\nOne-Hot encoded text:\",result.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
