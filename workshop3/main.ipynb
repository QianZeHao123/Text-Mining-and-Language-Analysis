{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\QianZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg  # Import the gutenberg corpus from NLTK\n",
    "import nltk  # Import the NLTK library\n",
    "nltk.download('gutenberg')  # Download the gutenberg corpus\n",
    "gutenberg.fileids()  # List file-ids in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars\tWords\tSents\tFile\n",
      "   7752\t 192427\t 887071\tausten-emma.txt\n",
      "   3747\t  98171\t 466292\tausten-persuasion.txt\n",
      "   4999\t 141576\t 673022\tausten-sense.txt\n",
      "  30103\t1010654\t4332554\tbible-kjv.txt\n",
      "    438\t   8354\t  38153\tblake-poems.txt\n",
      "   2863\t  55563\t 249439\tbryant-stories.txt\n",
      "   1054\t  18963\t  84663\tburgess-busterbrown.txt\n",
      "   1703\t  34110\t 144395\tcarroll-alice.txt\n",
      "   4779\t  96996\t 457450\tchesterton-ball.txt\n",
      "   3806\t  86063\t 406629\tchesterton-brown.txt\n",
      "   3742\t  69213\t 320525\tchesterton-thursday.txt\n",
      "  10230\t 210663\t 935158\tedgeworth-parents.txt\n",
      "  10059\t 260819\t1242990\tmelville-moby_dick.txt\n",
      "   1851\t  96825\t 468220\tmilton-paradise.txt\n",
      "   2163\t  25833\t 112310\tshakespeare-caesar.txt\n",
      "   3106\t  37360\t 162881\tshakespeare-hamlet.txt\n",
      "   1907\t  23140\t 100351\tshakespeare-macbeth.txt\n",
      "   4250\t 154883\t 711215\twhitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Chars\\tWords\\tSents\\tFile\")\n",
    "for fileid in gutenberg.fileids(): # Iterate through files in corpus\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    print(\"%7.0f\\t%7.0f\\t%7.0f\\t%s\" % (num_sents,num_words,num_chars,fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listofprocessedwords: ['the', 'new', 'table', 'is', 'red', 'the', 'blue', 'table', 'is', 'broken']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize  # Importtheword_tokenizefunctionfromNLTK\n",
    "from string import punctuation\n",
    "punctuation_list = list(punctuation)  # Convertstringwithpunctuationmarkstolist\n",
    "text = \"The new table is red. The blue table is broken.\"\n",
    "text_tokens_processed = []\n",
    "text_tokens = word_tokenize(text)  # Tokenisethetextintowords\n",
    "for token in text_tokens:  # Iteratethroughtheavailabletokens\n",
    "    if token not in punctuation_list:  # Omittokensthatarepunctuationmarks\n",
    "        # Addlowercaseversionoftokentolist\n",
    "        text_tokens_processed.append(token.lower())\n",
    "print(\"List of processed words:\", text_tokens_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the', 'new', 'red', 'table', 'is', 'broken', 'blue'}\n",
      "Vocabularysize: 7\n",
      "\n",
      "Vocabulary2: {'the', 'new', 'red', 'table', 'is', 'broken', 'blue'}\n",
      "Vocabulary2size: 7\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()  # Createanemptyset\n",
    "for word in text_tokens_processed:  # Iteratethroughavailablewords\n",
    "    vocabulary.add(word)  # Addwordtoset\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Vocabularysize:\", len(vocabulary))\n",
    "vocabulary2 = set(text_tokens_processed)\n",
    "print(\"\\nVocabulary2:\", vocabulary2)\n",
    "print(\"Vocabulary2size:\", len(vocabulary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7328\tausten-emma.txt\n",
      "  5820\tausten-persuasion.txt\n",
      "  6388\tausten-sense.txt\n",
      " 12755\tbible-kjv.txt\n",
      "  1521\tblake-poems.txt\n",
      "  3925\tbryant-stories.txt\n",
      "  1547\tburgess-busterbrown.txt\n",
      "  2622\tcarroll-alice.txt\n",
      "  8313\tchesterton-ball.txt\n",
      "  7780\tchesterton-brown.txt\n",
      "  6335\tchesterton-thursday.txt\n",
      "  8432\tedgeworth-parents.txt\n",
      " 17215\tmelville-moby_dick.txt\n",
      "  9007\tmilton-paradise.txt\n",
      "  3019\tshakespeare-caesar.txt\n",
      "  4703\tshakespeare-hamlet.txt\n",
      "  3451\tshakespeare-macbeth.txt\n",
      " 12437\twhitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():  # Iterate through documents in corpus\n",
    "    vocabulary_of_document = set()  # Create empty set\n",
    "    for word in gutenberg.words(fileid):  # Iterate through words in document\n",
    "        if word not in punctuation_list:  # Omit tokens that are punctuation marks\n",
    "            vocabulary_of_document.add(word.lower())\n",
    "    print(\"%6.0f\\t%s\" % (len(vocabulary_of_document), fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of Gutenberg corpus: 42314 words\n"
     ]
    }
   ],
   "source": [
    "vocabulary_of_corpus = set() # Create empty set\n",
    "\n",
    "for fileid in gutenberg.fileids(): # Iterate through documents in corpus\n",
    "    for word in gutenberg.words(fileid): # Iterate through words in document\n",
    "        if word not in punctuation_list: # Omit tokens that are punctuation marks\n",
    "            vocabulary_of_corpus.add(word.lower())\n",
    "\n",
    "print(\"Vocabulary of Gutenberg corpus:\",len(vocabulary_of_corpus),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'table' 'blue' 'red' 'broken' 'new' 'the'] \n",
      "\n",
      "[2 5 0 4 1 3 6] \n",
      "\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]] \n",
      "\n",
      "[0. 0. 1. 0. 0. 0. 0.] -> is\n",
      "[0. 0. 0. 0. 0. 1. 0.] -> table\n",
      "[1. 0. 0. 0. 0. 0. 0.] -> blue\n",
      "[0. 0. 0. 0. 1. 0. 0.] -> red\n",
      "[0. 1. 0. 0. 0. 0. 0.] -> broken\n",
      "[0. 0. 0. 1. 0. 0. 0.] -> new\n",
      "[0. 0. 0. 0. 0. 0. 1.] -> the\n"
     ]
    }
   ],
   "source": [
    "from numpy import array  # Import array type from numpy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "vocabulary = ['is', 'table', 'blue', 'red', 'broken', 'new', 'the']\n",
    "# Convert to array because it is required by the LabelEncoder() object\n",
    "data = array(vocabulary)\n",
    "print(data, \"\\n\")\n",
    "# Integer encoding- Assigns a unique index to each unique word\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(data)\n",
    "print(integer_encoded, \"\\n\")\n",
    "# One-Hot encoding- Assigns a One-Hot binary representation to each word\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded, \"\\n\")\n",
    "for i in range(len(data)):\n",
    "    print(onehot_encoded[i], \"->\", data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': array([0., 0., 1., 0., 0., 0., 0.]), 'table': array([0., 0., 0., 0., 0., 1., 0.]), 'blue': array([1., 0., 0., 0., 0., 0., 0.]), 'red': array([0., 0., 0., 0., 1., 0., 0.]), 'broken': array([0., 1., 0., 0., 0., 0., 0.]), 'new': array([0., 0., 0., 1., 0., 0., 0.]), 'the': array([0., 0., 0., 0., 0., 0., 1.])}\n",
      "\n",
      "red = [0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "for i in range(len(data)):\n",
    "    dictionary[data[i]] = onehot_encoded[i]\n",
    "\n",
    "print(dictionary)\n",
    "print(\"\\nred =\",dictionary['red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "def get_label_from_dictionary(dictionary, value):\n",
    "    for word, one_hot in dictionary.items():  # Iterate all pairs (word, one-hot representation) in the dictionary\n",
    "        if (one_hot == value).all():  # Compare equality between numpy arrays\n",
    "            return word\n",
    "\n",
    "\n",
    "print(get_label_from_dictionary(dictionary, [0, 0, 0, 0, 1, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of processed words: ['the', 'new', 'table', 'is', 'red', 'the', 'blue', 'table', 'is', 'broken']\n",
      "[0 0 0 0 0 0 0] OR [0. 0. 0. 0. 0. 0. 1.] = [0 0 0 0 0 0 1]\n",
      "[0 0 0 0 0 0 1] OR [0. 0. 0. 1. 0. 0. 0.] = [0 0 0 1 0 0 1]\n",
      "[0 0 0 1 0 0 1] OR [0. 0. 0. 0. 0. 1. 0.] = [0 0 0 1 0 1 1]\n",
      "[0 0 0 1 0 1 1] OR [0. 0. 1. 0. 0. 0. 0.] = [0 0 1 1 0 1 1]\n",
      "[0 0 1 1 0 1 1] OR [0. 0. 0. 0. 1. 0. 0.] = [0 0 1 1 1 1 1]\n",
      "[0 0 1 1 1 1 1] OR [0. 0. 0. 0. 0. 0. 1.] = [0 0 1 1 1 1 1]\n",
      "[0 0 1 1 1 1 1] OR [1. 0. 0. 0. 0. 0. 0.] = [1 0 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1] OR [0. 0. 0. 0. 0. 1. 0.] = [1 0 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1] OR [0. 0. 1. 0. 0. 0. 0.] = [1 0 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1] OR [0. 1. 0. 0. 0. 0. 0.] = [1 1 1 1 1 1 1]\n",
      "\n",
      "One-Hot encoded text: [1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from numpy import logical_or # Import the element-wise logical OR function from numpy\n",
    "from numpy import zeros # Import the zeros function from numpy\n",
    "text = \"The new table is red. The blue table is broken.\"\n",
    "print(\"List of processed words:\",text_tokens_processed)\n",
    "result = zeros(len(dictionary)) # Start from zero-valued vector- Convert to numpy array\n",
    "for word in text_tokens_processed: # Iterate words in text\n",
    "    print(result.astype(int), \"OR\", dictionary[word],\"= \",end='')\n",
    "    result = logical_or(result,dictionary[word]) # Compute the element-wise logical or between the partial result and the one-hot representation of the word\n",
    "    print(result.astype(int))\n",
    "print(\"\\nOne-Hot encoded text:\",result.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot encoded text: [0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def get_text_one_hot_encoding(words_list, dictionary):\n",
    "    # Start from zero-valued vector-Convert to numpy array\n",
    "    result = zeros(len(dictionary))\n",
    "    for word in words_list:  # Iterate words in text\n",
    "        # Compute the element-wise logical or between the partial result and the one-hot representation of the word\n",
    "        result = logical_or(result, dictionary[word])\n",
    "    return result.astype(int)\n",
    "\n",
    "\n",
    "words_list = ['the', 'broken', 'table']\n",
    "print(\"\\nOne-Hot encoded text:\", get_text_one_hot_encoding(words_list, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 7 samples and 10 outcomes> \n",
      "\n",
      "    1 blue\n",
      "    1 broken\n",
      "    2 is\n",
      "    1 new\n",
      "    1 red\n",
      "    2 table\n",
      "    2 the\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist  # Import the FreqDist function from NLTK\n",
    "text = \"The new table is red. The blue table is broken.\"\n",
    "text_tokens_processed = ['the', 'new', 'table', 'is',\n",
    "                         'red', 'the', 'blue', 'table', 'is', 'broken']\n",
    "vocabulary = {'new', 'broken', 'the', 'blue', 'table', 'red', 'is'}\n",
    "tf = FreqDist(text_tokens_processed)  # Compute term frequency of words\n",
    "print(tf, \"\\n\")\n",
    "vocabulary = sorted(vocabulary)  # Sort alphabetiacally for better presentation\n",
    "for word in vocabulary:\n",
    "    print(\"%5.0f %s\" % (tf[word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 1, 2, 2] -> The new table is red. The blue table is broken.\n",
      "[0, 1, 1, 1, 0, 1, 1] -> The new table is broken\n"
     ]
    }
   ],
   "source": [
    "text_tf = []\n",
    "for word in vocabulary:\n",
    "    text_tf.append(tf[word])\n",
    "print(text_tf, \"->\", text)\n",
    "text2 = \"The new table is broken\"\n",
    "text2_tokens_processed = ['the', 'new', 'table', 'is', 'broken']\n",
    "tf2 = FreqDist(text2_tokens_processed)  # Compute term frequency of words\n",
    "text2_tf = []\n",
    "for word in vocabulary:\n",
    "    text2_tf.append(tf2[word])\n",
    "print(text2_tf, \"->\", text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'new', 'table', 'is', 'red', 'the', 'blue', 'table', 'was', 'broken'], ['the', 'new', 'movie', 'that', 'we', 'watched', 'yesterday', 'was', 'terrible'], ['we', 'raised', 'the', 'red', 'and', 'blue', 'flag', 'yesterday']]\n",
      "\n",
      "Vocabulary: ['and', 'blue', 'broken', 'flag', 'is', 'movie', 'new', 'raised', 'red', 'table', 'terrible', 'that', 'the', 'was', 'watched', 'we', 'yesterday']\n",
      "\n",
      "Document frequencies: {'and': 1, 'blue': 2, 'broken': 1, 'flag': 1, 'is': 1, 'movie': 1, 'new': 2, 'raised': 1, 'red': 2, 'table': 1, 'terrible': 1, 'that': 1, 'the': 3, 'was': 2, 'watched': 1, 'we': 2, 'yesterday': 2}\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the lists of lowercase words without punctuation marks for each document\n",
    "texts_words_processed = []\n",
    "texts_words_processed.append(\n",
    "    ['the', 'new', 'table', 'is', 'red', 'the', 'blue', 'table', 'was', 'broken'])\n",
    "texts_words_processed.append(\n",
    "    ['the', 'new', 'movie', 'that', 'we', 'watched', 'yesterday', 'was', 'terrible'])\n",
    "texts_words_processed.append(\n",
    "    ['we', 'raised', 'the', 'red', 'and', 'blue', 'flag', 'yesterday'])\n",
    "print(texts_words_processed)\n",
    "# Create the vocabulary\n",
    "vocabulary_texts = set()\n",
    "for doc in texts_words_processed:\n",
    "    for word in doc:\n",
    "        vocabulary_texts.add(word)\n",
    "# Sort vocabulary alphabetically for better presentation\n",
    "vocabulary_texts = sorted(vocabulary_texts)\n",
    "print(\"\\nVocabulary:\", vocabulary_texts)\n",
    "DF = dict()  # Create an empty dictionary\n",
    "for word in vocabulary_texts:  # Iterate through words in vocabulary\n",
    "    cnt = 0\n",
    "    for doc in texts_words_processed:  # Iterate through documents\n",
    "        if word in doc:\n",
    "            cnt += 1  # cnt += 1 is equal to cnt = cnt + 1\n",
    "    DF[word] = cnt\n",
    "print(\"\\nDocument frequencies:\", DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "$ IDF(t,D) = \\log (\\frac{N}{DF(t,d)}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 1.0986122886681098, 'is': 1.0986122886681098, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 1.0986122886681098, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "import math  # Import math library\n",
    "N = 3  # The corpus contains 3 documents\n",
    "IDF = dict()  # Create an empty dictionary\n",
    "for word in vocabulary_texts:  # Iterate through words in vocabulary\n",
    "    IDF[word] = math.log(N / DF[word])  # Compute IDF of word\n",
    "print(\"IDF:\", IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency- Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "$ TFIDF(t,d,D) = TF(t,d) \\times IDF(t,D) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FreqDist({'the': 2, 'table': 2, 'new': 1, 'is': 1, 'red': 1, 'blue': 1, 'was': 1, 'broken': 1}), FreqDist({'the': 1, 'new': 1, 'movie': 1, 'that': 1, 'we': 1, 'watched': 1, 'yesterday': 1, 'was': 1, 'terrible': 1}), FreqDist({'we': 1, 'raised': 1, 'the': 1, 'red': 1, 'and': 1, 'blue': 1, 'flag': 1, 'yesterday': 1})] \n",
      "\n",
      "Text 0 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 1 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 2 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 3 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 4 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 5 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 6 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 7 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 8 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 9 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 10 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 11 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 12 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 13 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 14 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 15 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 16 TF-IDF: {'and': 0.0, 'blue': 0.4054651081081644, 'broken': 1.0986122886681098, 'flag': 0.0, 'is': 1.0986122886681098, 'movie': 0.0, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.4054651081081644, 'table': 2.1972245773362196, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 0.0, 'we': 0.0, 'yesterday': 0.0} \n",
      "\n",
      "Text 17 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 18 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 19 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 20 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 21 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 22 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 23 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 24 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 25 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 26 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 27 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 28 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 29 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 30 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 31 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 32 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 33 TF-IDF: {'and': 0.0, 'blue': 0.0, 'broken': 0.0, 'flag': 0.0, 'is': 0.0, 'movie': 1.0986122886681098, 'new': 0.4054651081081644, 'raised': 0.0, 'red': 0.0, 'table': 0.0, 'terrible': 1.0986122886681098, 'that': 1.0986122886681098, 'the': 0.0, 'was': 0.4054651081081644, 'watched': 1.0986122886681098, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 34 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 35 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 36 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 37 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 38 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 39 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 40 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 41 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 42 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 43 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 44 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 45 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 46 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 47 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 48 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 49 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n",
      "Text 50 TF-IDF: {'and': 1.0986122886681098, 'blue': 0.4054651081081644, 'broken': 0.0, 'flag': 1.0986122886681098, 'is': 0.0, 'movie': 0.0, 'new': 0.0, 'raised': 1.0986122886681098, 'red': 0.4054651081081644, 'table': 0.0, 'terrible': 0.0, 'that': 0.0, 'the': 0.0, 'was': 0.0, 'watched': 0.0, 'we': 0.4054651081081644, 'yesterday': 0.4054651081081644} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist  # Import the FreqDist function from NLTK\n",
    "TF = []\n",
    "for doc in texts_words_processed:  # Iterate though documents\n",
    "    TF.append(FreqDist(doc))  # Compute word frequency\n",
    "print(TF, \"\\n\")\n",
    "TFIDF = []\n",
    "for tf_doc in TF:\n",
    "    tfidf_doc = dict()\n",
    "    for word in vocabulary_texts:  # Iterate through words in vocabulary\n",
    "        # Compute TF-IDF- tf_doc is of type FreqDist and will return 0 for words that don't exist\n",
    "        tfidf_doc[word] = tf_doc[word] * IDF[word]\n",
    "        TFIDF.append(tfidf_doc)\n",
    "cnt = 0\n",
    "for tfidf_doc in TFIDF:\n",
    "    print(\"Text\", cnt, \"TF-IDF:\", tfidf_doc, \"\\n\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
